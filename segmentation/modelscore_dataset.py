"""
Loads images from GTA translated into Cityscapes along with corresponding GTA labels. Used for testing.
"""
import os
import scipy.io
import collections
import numpy as np
from PIL import Image

import torch.utils.data as data
from torchvision import transforms
from torchvision.transforms.functional import to_pil_image

class GTAModelScoreDataset(data.Dataset):
    '''
    Validation set: GTA testing images. Images are translations generated by model and ground truth are synth labels.
    '''

    def __init__(self, resolution, adaptation):
        if adaptation == 'cyclegan':
            self.images_root = "/raid/cristinam/da_kpn_experiments/translations/cyclegan_gta_translations_regenerated/cityscapes_gta_cyclegan_id1/test_35/images" # Translations from CycleGAN
        elif adaptation == 'da_kpn':
            self.images_root = "/raid/cristinam/da_kpn_experiments/translations/da_kpn_no_encoder_ep31_translations_gta_regenerated" # Translations from DA-KPN
        elif adaptation == 'da_kpn_source_enc':
            self.images_root = "/raid/cristinam/da_kpn_experiments/translations/da_kpn_source_encoder_iter103000_translations_gta"
        elif adaptation == 'da_kpn_source_noaffine':
            self.images_root = "/raid/cristinam/da_kpn_experiments/translations/da_kpn_source_ablation_noaffine"
        elif adaptation == 'da_kpn_source_noblur':
            self.images_root = "/raid/cristinam/da_kpn_experiments/translations/da_kpn_source_ablation_noblur"
        elif adaptation == 'da_kpn_source_nonoise':
            self.images_root = "/raid/cristinam/da_kpn_experiments/translations/da_kpn_source_ablation_nonoise"
        elif adaptation == 'cut':
            self.images_root = "/raid/cristinam/contrastive-unpaired-translation/results/cityscapes_CUT/train_latest/images/fake_B"
        elif adaptation == 'fastcut':
            self.images_root = "/raid/cristinam/contrastive-unpaired-translation/results/cityscapes_FastCUT/train_latest/images/fake_B"
        elif adaptation == 'vsait':
            self.images_root = "/raid/cristinam/vsait/checkpoints/gta_vsait_adapt/images"
            #self.images_root = "/raid/cristinam/vsait/checkpoints/vsait_adapt/images" # high res trained
        elif adaptation == 'source':
            self.images_root = "/raid/datasets/GTA/images" # Source
        else:
            print("Adaptation ", adaptation, " not added to dataset")
            raise NotImplementedError
        self.labels_root = "/raid/datasets/GTA/labels"
        self.resolution = resolution
        self.files = collections.defaultdict(list)

        ids = scipy.io.loadmat('/raid/datasets/GTA/split.mat')['testIds']
        for id in ids:
            if adaptation == 'cyclegan':
                image = os.path.join(self.images_root, str(id[0]).zfill(5)+'_fake.png')
            else:
                image = os.path.join(self.images_root, str(id[0]).zfill(5)+'.png')
            label = os.path.join(self.labels_root, str(id[0]).zfill(5)+'.png')
            self.files['val'].append({
                "image": image,
                "label": label
            })

    def __len__(self):
        return len(self.files['val'])

    def __getitem__(self, index):
        img_transform = transforms.Compose([
            transforms.ToTensor(),
            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
        ])

        datafiles = self.files['val'][index]
        img = Image.open(datafiles["image"]).convert('RGB')
        img = img.resize(self.resolution[::-1])
        img = img_transform(img)

        lb = Image.open(datafiles["label"]).convert('P')
        lb = lb.resize(self.resolution[::-1])
        lb = np.array(lb)

        # Convert gta labels to cityscapes
        final_lb = np.zeros(self.resolution)
        id_to_trainid = {
            7: 0,
            8: 1,
            11: 2,
            12: 3,
            13: 4,
            17: 5,
            19: 6,
            20: 7,
            21: 8,
            22: 9,
            23: 10,
            24: 11,
            25: 12,
            26: 13,
            27: 14,
            28: 15,
            31: 16,
            32: 17,
            33: 18
        }
        unique = np.unique(lb)
        for c in unique:
            final_lb[lb == c] = id_to_trainid.get(c, 0)

        return img, final_lb, datafiles["image"]

class FaceSyntheticsModelScoreDataset(data.Dataset):
    '''
    Validation set: FaceSynthetics testing images. Images are translations generated by model and ground truth are synth labels.
    '''

    def __init__(self, resolution, adaptation):
        if adaptation == 'cyclegan':
            self.images_root = "/raid/cristinam/pytorch-CycleGAN-and-pix2pix/results/celeba_facesynthetics/test_latest/images" # Translations from CycleGAN
        elif adaptation == 'da_kpn':
            self.images_root = "/raid/cristinam/da_kpn_experiments/faceparsing/da_kpn_no_encoder_iter_67000_translations_facesynthetics" # Translations from DA-KPN
        elif adaptation == 'vsait':
            self.images_root = "/raid/cristinam/vsait/checkpoints/vsait_adapt/images" # Translations from VSAIT
        elif adaptation == 'source':
            self.images_root = "/raid/datasets/FaceSynthetics" # Source
        else:
            print("Translation set ", adaptation, " not implemented")
            raise NotImplementedException
        self.labels_root = "/raid/datasets/FaceSynthetics"
        self.resolution = resolution
        self.files = collections.defaultdict(list)

        if adaptation == 'source':
            ids = [f for f in os.listdir(self.images_root) if len(f) == 10]
            for id in ids:
                self.files['val'].append({
                    "image": os.path.join(self.images_root, id),
                    "label": os.path.join(self.labels_root, id.split('.')[0] + '_seg.png')
                })
        elif adaptation == 'cyclegan':
            ids = [f for f in os.listdir(self.images_root) if f[-9:] == '_fake.png']
            for id in ids:
                self.files['val'].append({
                    "image": os.path.join(self.images_root, id),
                    "label": os.path.join(self.labels_root, id.split('_')[0]+'_seg.png')
                })
        else:
            ids = [f for f in os.listdir(self.images_root)]
            for id in ids:
                self.files['val'].append({
                    "image": os.path.join(self.images_root, id),
                    "label": os.path.join(self.labels_root, id.split('.')[0]+'_seg.png')
                })

        self.ignore_label = 255
        self.facesynthetics_classes = ['skin', 'nose', 'right_eye', 'left_eye', 'right_brow']
        self.id2label = {0: self.ignore_label, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5, 6: 6, 7: 7, 8: 8, 9: 9, 10: 10, 11: 11, 12: 12,
                    13: 13,
                    14: 14, 15: 15, 16: 16, 17: 17, 18: 18}
        self.color_list = [[0, 0, 0], [204, 0, 0], [76, 153, 0], [204, 204, 0], [51, 51, 255], [204, 0, 204], [0, 255, 255],
                      [255, 204, 204], [102, 51, 0], [255, 0, 0], [102, 204, 0], [255, 255, 0], [0, 0, 153],
                      [0, 0, 204],
                      [255, 51, 153], [0, 204, 204], [0, 51, 0], [255, 153, 51], [0, 204, 0]]
        self.celebA_classes = ['skin', 'nose', 'eye_g', 'l_eye', 'r_eye', 'l_brow', 'r_brow', 'l_ear', 'r_ear', 'mouth',
                          'u_lip',
                          'l_lip', 'hair', 'hat', 'ear_r', 'neck_l', 'neck', 'cloth']

    def __len__(self):
        return len(self.files['val'])

    def remap_labels_to_train_ids(self, arr):
        out = self.ignore_label * np.ones(arr.shape, dtype=np.uint8)
        for id, label in self.id2label.items():
            out[arr == id] = int(label)
        return out

    def __getitem__(self, index):
        img_transform = transforms.Compose([
            transforms.ToTensor(),
            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
        ])

        datafiles = self.files['val'][index]
        img = Image.open(datafiles["image"]).convert('RGB')
        img = img.resize(self.resolution[::-1])
        img = img_transform(img)

        lb = Image.open(datafiles["label"]).convert('P')
        lb = lb.resize(self.resolution[::-1])
        lb = np.array(lb)
        lb = self.remap_labels_to_train_ids(lb)

        return img, lb, datafiles["image"]